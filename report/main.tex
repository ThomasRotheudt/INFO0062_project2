\documentclass[a4paper,10pt]{article}
%%%%%%%%%%% Package %%%%%%%%%%%
\usepackage{amssymb}  % Used for math symbols.
\usepackage{amsmath}  % Used for math environments.
\usepackage[french,english]{babel} % Used to define the language.
\usepackage{datetime} % Used for the dynamic date.
\usepackage{float}    % Used to force figure to stay on place with [H]
\usepackage[T1]{fontenc} % Used to display french character correctly.
\usepackage{geometry} % Used to change margin.
\usepackage{graphicx} % Used to display image/graphic.
\usepackage{hyperref} % Used for hypertext.
\usepackage{inputenc} % Used to specify encodage.
\usepackage{listings} % Used to display code.
\usepackage{lipsum}   % Used to generate lorem ipsum
\usepackage{setspace} % Used to define space between lines.
\usepackage{tabularx} % Used for some table

%%%%%%%%%%% Param %%%%%%%%%%%
\geometry{top=2.4cm, bottom=2.4cm, left=2.4cm , right=2.4cm}
\hypersetup{
    colorlinks=true,            % Color link instead of framing links.
    linkcolor=blue,             % Color of intern link.
    urlcolor=blue,              % Color of url link.
}
\inputencoding{utf8}            % Define the encoding as utf8.
\lstset{
  language={}, % Aucun langage spécifique
  basicstyle=\tt\footnotesize, % Police de caractères // \tt\footnotesize
  numbers=left, % Numérotation des lignes à gauche
  numberstyle=\tiny, % Style de numérotation des lignes // \tiny\color{mygray}
  frame=single, % Encadrement du code
  breaklines=true, % Saut de ligne automatique
  showstringspaces=false % Ne pas afficher les espaces dans les chaînes de caractères
}
\setlength{\parindent}{15pt}    % Define indentation size (default=15pt)
\setstretch{1}                  % Define space between lines (default=1)

%%%%%%%%%%% Useful link %%%%%%%%%%%
% https://www.tablesgenerator.com/ % Useful to create LaTeX table
 
%%%%%%%%%%% Commands %%%%%%%%%%%
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Print a line
\newcommand{\schoolyear}{\the\numexpr\year-(\ifnum\month<9 1\else 0\fi)\relax-\the\numexpr\year+(\ifnum\month<9 0\else 1\fi)} % Print the current academic year.

%%%%%%%%%%% Document %%%%%%%%%%%
\begin{document}

%%%%%%%%%%% Front page %%%%%%%%%%%
\begin{titlepage}
   \begin{center}

    % Upper part of the page. The '~' is needed because \\
    % only works if a paragraph has started.
    ~\\[4cm]
    \includegraphics[scale=0.9]{images/facsa.png}
    ~\\[1.5cm]
    % Title
    \HRule \\[0.4cm]
    {\huge Introduction to Machine Learning\\[0.4cm] }

    \HRule \\[1cm]
    
    \textsc{\Large Bias and variance analysis}\\[2cm]
    \vspace{2cm}
    % Author and supervisor
    \begin{minipage}{0.3\textwidth}
      \begin{flushleft} \large
        Arthur \textsc{GRAILLET}\\
        Robin \textsc{FONBONNE}\\
        Thomas \textsc{ROTHEUDT}
      \end{flushleft}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
      \begin{flushright}\large
        s182019\\
        s182200\\
        s191895
      \end{flushright}
    \end{minipage}

    \vfill

    % Bottom of the page
    {\large Academic year \schoolyear}

  \end{center}
\end{titlepage}
\newpage

%%%%%%%%%%% Report %%%%%%%%%%%

%%% Question 1: Analytical derivations
\section{Analytical derivations}
\begin{enumerate}
    %% Question 1.1: Show that the expected generalization error...
    \item 
    We have the expected generalization error of the k-Nearest Neighbours algorithm:
    $$
    E_{LS}\{E_{y\mid \textbf{x}}\{(y - \hat{y}(\textbf{x};LS,k))^2\}\}
    $$
    
    We can substitude $y$ by $f(\textbf{x}) + \epsilon$ and expand the square in the expected squared error written as
    $$
    E_{y\mid \textbf{x}}\{(y - \hat{y}(\textbf{x};LS,k))^2\}
    $$
    to obtain:
    $$
    E_{y\mid \textbf{x}}\{(f(\textbf{x}) - \hat{y}(\textbf{x};LS,k))^2 + 2\epsilon(f(\textbf{x})-\hat{y}(\textbf{x};LS,k)) + \epsilon^2\}
    $$

    Since $E[\epsilon] = 0$ and $\epsilon$ is independent of $f(x)$ and $\hat{y}(\textbf{x};LS,k)$ the cross-term vanishes and the formula becomes:
    $$
    E_{y\mid \textbf{x}}\{(y - \hat{y}(\textbf{x};LS,k))^2\} = (f(\textbf{x})-\hat{y}(\textbf{x};LS,k))^2 + E[\epsilon^2]
    $$

    Since $\epsilon \sim \mathcal{N}(0, \sigma^2)$, we have $E[\epsilon^2] = \sigma^2$ it can be written as:
    $$
    (f(\textbf{x})-\hat{y}(\textbf{x};LS,k))^2 + \sigma^2
    $$

    We can rewrite $\hat{y}(\textbf{x};LS,k)$ as the average of the function values at the k-nearest neighbors:
    $$
    \hat{y}(\textbf{x};LS, k) = \frac{1}{k}\sum^k_{l=1}y_{(l)}
    $$
    where $y_{(l)} = f(\textbf{x}_{(l)}) + \epsilon$ (with $\epsilon \sim \mathcal{N}(0, \sigma^2)$). Therefore it can be written as:
    $$
    \hat{y}(\textbf{x};LS, k) = \frac{1}{k}\sum^k_{l=1}f(\textbf{x}_{(l)}) + \epsilon = \frac{1}{k}\sum^k_{l=1}f(\textbf{x}_{(l)}) + \frac{1}{k}\sum^k_{l=1}\epsilon
    $$

    The expected squared error can be decomposed as follows:
    $$
    \sigma^2  
    + \left(f(\textbf{x})-\frac{1}{k}\sum^k_{l=1}f(\textbf{x}_{(l)})\right)^2
    + 2\left(f(\textbf{x})-\frac{1}{k}\sum^k_{l=1}f(\textbf{x}_{(l)})\right) \left(\frac{1}{k}\sum^k_{l=1}\epsilon\right)
    + \left(\frac{1}{k}\sum^k_{l=1}\epsilon\right)^2
    $$
    Since $\epsilon$ is an independent random variable with a mean of zero, the cross-term has an expectation of zero. We can write the expected generalization error as:
    $$
    E_{LS}\{E_{y\mid \textbf{x}}\{(y - \hat{y}(\textbf{x};LS,k))^2\}\} =
    E_{LS}\left(\sigma^2\right)
    + E_{LS}\left[\left(f(\textbf{x})-\frac{1}{k}\sum^k_{l=1}f(\textbf{x}_{(l)})\right)^2\right]
    + E_{LS}\left[\left(\frac{1}{k}\sum^k_{l=1}\epsilon\right)^2\right]
    $$

    The expectation of a constant is the constant itself and the expectation of the third term
    $$
    E_{LS}\left[\left(\frac{1}{k}\sum^k_{l=1}\epsilon\right)^2\right] = \frac{\sigma^2}{k}
    $$
    because $\text{Var}(\epsilon) = \sigma^2$.

    Since \textbf{x} is fixed, the second term is also fixed. Taking the expectation $E_{LS}$ over this term has no effect.

    Therefore we can conclude that
    $$
    E_{LS}\{E_{y\mid \textbf{x}}\{(y - \hat{y}(\textbf{x};LS,k))^2\}\} =
    \sigma^2
    + \left[f(\textbf{x})-\frac{1}{k}\sum^k_{l=1}f(\textbf{x}_{(l)})\right]^2
    + \frac{\sigma^2}{k}
    $$

    The first term represents the noise, the second the bias, and the third the variance. 

    %% Question 1.2: Let us now assume that the problem is unidimensional...
    \item 
    We know that $f(x) = x^2$. Since we have to evaluate the bias and variance at $x = 0$ we know that $f(x) = 0$.

    The bias can be written as:
    $$
    \text{bias} = \frac{-1}{k}\sum^k_{l=1}({x}_{(l)})^2
    $$
    Since the training inputs are symmetrically distributed around $x=0$ on a uniform grid in [-1,+1], the k-neighbors of $x=0$ include the point $x=0$, $k'$ positive points, and $k'$ negative points. We have that
    $$
    \sum^k_{l=1}({x}_{(l)})^2 = 2\sum^{k'}_{l=1}\left(\frac{i}{N'}\right)^2 = \frac{2}{(N')^2}\sum^{k'}_{l=1}i^2
    $$
    We can use the formula to write the sum as a function of $k'$:
    $$
    \frac{2}{(N')^2}\sum^{k'}_{l=1}i^2 = \frac{2}{(N')^2}\frac{k'(k'+1)(2k'+1)}{6}
    $$
    The result must be expressed as a function of k and N, we can replace $k'$ and $N'$ with the relation $k' = \frac{k-1}{2}$ and $N' = \frac{N-1}{2}$:
    $$
    \frac{2}{\frac{(N-1)^2}{4}}\frac{(\frac{k-1}{2})(\frac{k-1}{2}+1)(k-1+1)}{6} = \frac{k(k^2 - 1)}{3(N-1)^2}
    $$
    The bias can be written as a function of k and N:
    $$
    \text{bias} = \frac{-(k^2 - 1)}{3(N-1)^2}
    $$


    The variance is already expressed as a function of $\sigma$ and $k$:
    $$
    \text{variance} = \frac{\sigma^2}{k}
    $$

    
    %% Question 1.3: Discuss the impact of N, k, an σ on bias and variance. Are there 
    % some surprising or missing dependences? If so, try and explain them.
    \item 
    \begin{itemize}
        \item 
        $k$ appears in the formula of the variance and it is obvious that a greater k leads to a smaller variance. But increasing $k$ may lead to an increase of the bias\footnote{We use increase and decrease to define the change on the bias absolute value.} because we would consider more distant points which reduce the flexibility of the model.
        \item
        Increasing the size $N$ of the learning sample generally leads to a denser distribution, therefore with the same $k$ and $\sigma$, the k-nearest neighbors around $x$ will be closer to $x$ and we can expect that they better represent the local behavior of $f(x)$.
        We can say that larger N leads to smaller bias. If we consider the problem explain in 2, we can see that the size $N$ is in the bias formula and increasing $N$ reduces the bias.

        $N$ has no direct impact on the variance but we can note that a larger $N$ allows a larger $k$ which leads to a lower variance. 
        \item 
        A greater $\sigma$ means more noise and as we may expect it increases the variance but does not impact the bias.
    \end{itemize}


    
    %% Question 1.4: For all combinations of N in {25, 50} and σ ∈ {0.0, 0.1, 0.2}, determine 
    % the value k∗ of k that minimizes the expected generalization error at x = 0.
    \item
    % New attempt at derivative approach:
    % As suggested we'll compute the minimum of the bias plus variance found in 2. Let's define:
    % $$
    % f(k) = \sigma^2 + \left[\frac{-(k^2 - 1)}{3(N-1)^2}\right]^2 + \frac{\sigma^2}{k}
    % $$
    % To find the minimum we should first compute the derivative $f'(k)$:
    % $$
    % f'(k) = \frac{4k(k^2-1)}{9(N-1)^4} - \frac{\sigma^2}{k^2}
    % $$
    % The term with the greastest ordre of $k$ (third order) in $f(k)$ is positive and $f(k)$ is continuous, 
    % therefore the minimum occurs when the derivative is null.
    % $$
    % f'(k) = 0
    % $$
    % We find:
    % $$
    % k = pas trouvé
    % $$
    % This doesn't take into account that k must be an integer nor the fact that the formula were found assuming k being odd.

    % The results using the derivative are the following:
    % \begin{table}[H]
    %   \centering
    %   \begin{tabular}{l|l|l|l|}
    %   \cline{2-4}
    %                            & $\sigma = 0$ & $\sigma = 0.1$ & $\sigma = 0.2$ \\ \hline
    %   \multicolumn{1}{|l|}{$N = 25$} & 0  &  1,423  &  2,258 \\ \hline
    %   \multicolumn{1}{|l|}{$N = 50$} & 0  &  2,290   &  3,635 \\ \hline
    %   \end{tabular}
    %   \caption{Table of $k^*$ based on derivative}
    % \end{table}
    % These values cannot be used like that for as an hyperparameter. We have to consider at least one neighbor and the float must be rounded to integer.

    % If we compute the best $k*$ using experiments with only odd values of $k$ we find the following results:
    % \begin{table}[H]
    %   \centering
    %   \begin{tabular}{l|l|l|l|}
    %   \cline{2-4}
    %                            & $\sigma = 0$ & $\sigma = 0.1$ & $\sigma = 0.2$ \\ \hline
    %   \multicolumn{1}{|l|}{$N = 25$} & 1  &  3  &  3 \\ \hline
    %   \multicolumn{1}{|l|}{$N = 50$} & 1  &  3   &  5 \\ \hline
    %   \end{tabular}
    %   \caption{Table of $k^*$ based on experiments}
    % \end{table}




    As suggested we'll compute the minimum by running actual experiments. We have to minimize this function:
    $$
    f(k) = \sigma^2 + \left[\frac{-(k^2 - 1)}{3(N-1)^2}\right]^2 + \frac{\sigma^2}{k}
    $$
    By testing every possible value of $k$ such that $k = 2k' + 1$ with $0 \le k' \le \frac{N-1}{2}$ for each combination of $N$ and $\sigma$ we have the following results:
    \begin{table}[H]
      \centering
      \begin{tabular}{l|l|l|l|}
      \cline{2-4}
                               & $\sigma = 0$ & $\sigma = 0.1$ & $\sigma = 0.2$ \\ \hline
      \multicolumn{1}{|l|}{$N = 25$} & 1  &  5  &  7 \\ \hline
      \multicolumn{1}{|l|}{$N = 50$} & 1  &  11   &  13 \\ \hline
      \end{tabular}
      \caption{Table of $k^*$ considering only odd values for k}
    \end{table}
    The cells represent the $k^*$ for each combination.

    If we can still consider even values for $k$ with the formula obtained considering only odd values of $k$ we would have:
    \begin{table}[H]
      \centering
      \begin{tabular}{l|l|l|l|}
      \cline{2-4}
                               & $\sigma = 0$ & $\sigma = 0.1$ & $\sigma = 0.2$ \\ \hline
      \multicolumn{1}{|l|}{$N = 25$} & 1  &  6  &  8 \\ \hline
      \multicolumn{1}{|l|}{$N = 50$} & 1  &  11   &  14 \\ \hline
      \end{tabular}
      \caption{Table of $k^*$ considering every value for $k$}
    \end{table}


    
    % Below was a attempt for the derivative approach.
    % As suggested we'll compute the minimum of the bias plus variance found in 2. Let's define:
    % $$
    % f(k) = \left(\frac{4(k-1)(\frac{k-1}{2}+1)}{3(N-1)^2}\right)^2 + \frac{\sigma^2}{k}
    % $$
    % To find the minimum we should first compute the derivative $f'(k)$. For a simplier computation we will write:
    % $$
    % f(k) = \frac{4}{{3(N-1)^2}} \left(\frac{(k-1)^2}{2} + (k-1)\right)^2 + \frac{\sigma^2}{k}
    % $$
    % Now we can compute the derivative:
    % $$
    % f'(k) = \frac{4}{{3(N-1)^2}} 2\left(\frac{(k-1)^2}{2} + (k-1)\right)\left(\frac{2(k-1)}{2} + 1\right) - \frac{\sigma^2}{k^2}
    % $$
    % $$
    % = \frac{8}{{3(N-1)^2}} \left(\frac{(k-1)^2}{2} + (k-1)\right)((k-1) + 1) - \frac{\sigma^2}{k^2}
    % $$
    % $$
    % = \frac{4}{{3(N-1)^2}} (k-1) (k^2 + k - 1) - \frac{\sigma^2}{k^2}
    % $$
    % To find the minimum we must solve the equation ($f'(k) = 0$):
    % $$
    % {k^2}(k-1) (k^2 + k - 1) = \frac{3\sigma^2(N-1)^2}{4}
    % $$

    %% Question 1.5: Discuss the impact of N and σ on k∗.
    \item 
    Increasing the size of $N$ or $\sigma$ tends to increase the value of $k^*$. If $\sigma = 0$ then modifying $N$ won't change $k^*$ because the best value would always be to consider only one element.
\end{enumerate}

%%% Question 2: Empirical analysis
\section{Empirical analysis}
    %% Question 2.1: Explain why estimating the residual error term is very difficult in this setting.
The residual error represents the minimal attainable error, which is the irreducible noise in the data. The key challenge is that we don't know the true function $f^*(x)$ that maps wine characteristics to quality scores. Without this true function $f^*(x)$, we cannot measure how much of the prediction error comes from noise versus other sources of error. \\
    
    This is because the residual error is defined as the difference between the actual observed values y and the true function values \[ f^*(x): \mathbb{E}[(y - f^*(x))^2]\]\\
    Even if we build a very good model, we cannot know how close it is to this true underlying function f*(x), and therefore cannot isolate how much of the error is truly irreducible (residual error) versus due to model imperfections. \\
    
    In our case, this is particularly challenging because the relationship between wine characteristics and quality involves human judgments that do not follow any consistent mathematical function, making the true $f^*(x)$ even more difficult to determine. \\
    
    Additionally, the observed error is a combination of bias, variance, and residual error. \[ Total Error = Bias + Variance + ResidualError\] Since bias and variance arise from model imperfections, separating these components from the irreducible residual error becomes nearly impossible without further assumptions or prior knowledge of the noise distribution.
    

    %% Question 2.2: Describe a protocol to nevertheless estimate variance, the expected error, as 
    % well as the sum of the bias and the residual error from a pool P.
Our protocol consists of the following steps : \\ 

\textbf{Step 1}
\begin{enumerate}
    \item From the dataset \( P \) of size \( N_S \), randomly divide the data into multiple subsets of fixed size \( N \) (learning samples, \( LS \)).
    \item Select a fixed, independent test set \( TS \) for evaluating the model performance. The test set must remain unchanged and independent of the learning samples.
\end{enumerate}

\textbf{Step 2}
\begin{enumerate}
    \item For each learning sample \( LS_j \):
    \begin{enumerate}
        \item Train a model \( \hat{f}_{LS_j} \) using the learning sample \( LS_j \).
        \item Generate predictions on the test set \( TS \) for each trained model.
        \item Record the predictions for each point \( x_i \in TS \).
    \end{enumerate}
\end{enumerate}
\newpage
\textbf{Step 3} 
For every point \( x_i \in TS \), decompose the total error into variance, bias, and residual error:

\begin{enumerate}
    \item \textbf{Expected Error (Total Error):}
    \[
    \text{Total Error} = \frac{1}{M} \sum_{j=1}^M (y_i - \hat{f}_{LS_j}(x_i))^2
    \]
    where \( M \) is the number of models trained using different learning samples \( LS_j \), and \( y_i \) is the true value for \( x_i \).

    \item \textbf{Variance:}
    \[
    \text{Variance} = \frac{1}{M} \sum_{j=1}^M (\hat{f}_{LS_j}(x_i) - \bar{\hat{f}}(x_i))^2
    \]
    where \( \bar{\hat{f}}(x_i) = \frac{1}{M} \sum_{j=1}^M \hat{f}_{LS_j}(x_i) \) is the mean of the predictions across different learning samples.

    \item \textbf{Sum of Bias and Residual Error:}
    \[
    \text{Bias + Residual Error} = \text{Total Error} - \text{Variance}
    \]
    Since the residual error is constant across different models, any variation in the total error after subtracting variance can be attributed to the bias and residual error.
\end{enumerate}

\textbf{Step 4}
\begin{enumerate}
    \item For each point \( x_i \in TS \), calculate the average values of the variance, bias + residual error, and total error across all the models.
    \item Use these average values as the overall estimates for variance, bias + residual error, and total error.
\end{enumerate}


\vspace{0.4cm}

This protocol can be repeated with different hyperparameter settings of the learning algorithm to assess their impact on bias and variance. Since the residual error remains constant, any changes in the decomposed values can be attributed to changes in either bias or variance, allowing us to understand how different hyperparameter choices affect the bias-variance tradeoff.


    
    %% Question 2.3: Implement and use this protocol on the given dataset to estimate the expected error, 
    % variance, and the sum of bias and residual error, for Lasso regression, kNN, and regression trees.

    
    %% Question 2.4: For the same three methods, show the impact of the learning sample size on bias and 
    % variance.

    
    %% Question 2.5: Two so-called ensemble methods to address variance and bias are bagging 
    % ("bootstrap aggregating") and boosting. 

    
\end{document}